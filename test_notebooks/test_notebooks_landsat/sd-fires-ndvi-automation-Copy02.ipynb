{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"earth-lab-logo-rgb.png\" width=\"150\" height=\"150\" />\n",
    "\n",
    "# Homework Template: Earth Analytics Python Course: Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting this assignment, be sure to restart the kernel and run all cells. To do this, pull down the Kernel drop down at the top of this notebook. Then select **restart and run all**.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below.\n",
    "\n",
    "* IMPORTANT: Before you submit your notebook, restart the kernel and run all! Your first cell in the notebook should be `[1]` and all cells should run in order! You will lose points if your notebook does not run. \n",
    "\n",
    "For all plots and code in general:\n",
    "\n",
    "* Add appropriate titles to your plot that clearly and concisely describe what the plot shows (e.g. time, location, phenomenon).\n",
    "* Be sure to use the correct bands for each plot.\n",
    "* Specify the source of the data for each plot using a plot caption created with `ax.text()`.\n",
    "* Place ONLY the code needed to create a plot in the plot cells. Place additional processing code ABOVE that cell (in a separate code cell).\n",
    "\n",
    "Make sure that you:\n",
    "\n",
    "* **Only include the package imports, code, data, and outputs that are CRUCIAL to your homework assignment.**\n",
    "* Follow PEP 8 standards. Use the `pep8` tool in Jupyter Notebook to ensure proper formatting (however, note that it does not catch everything!).\n",
    "* Keep comments concise and strategic. Don't comment every line!\n",
    "* Organize your code in a way that makes it easy to follow. \n",
    "* Write your code so that it can be run on any operating system. This means that:\n",
    "   1. the data should be downloaded in the notebook to ensure it's reproducible.\n",
    "   2. all paths should be created dynamically using the os package to ensure that they work across operating systems. \n",
    "* Check for spelling errors in your text and code comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:13.863275Z",
     "start_time": "2020-05-04T21:06:13.860258Z"
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"Robina Shaheen\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Colored Bar](colored-bar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work flow to automate NDVI for NEON sites (HARV, SJER)\n",
    "**Goal**: Automate Normalized Difference Vegetation Indices using Landsat8 multispectral bands for an entire year without clouds.\n",
    "1.\tImport packages required for data processing and plotting.\n",
    "2.\tDownload data using earthpy and set working directory to earth-analytics.\n",
    "3.\tDefine pixel values to mask clouds and shadows. Define a masking variable to be used later.\n",
    "4.\tDefine a function to crop images, produce numpy array, metadata, remove clouds and calculate mean ndvi values for each image.\n",
    "    * i.\tContext manager will be used in conjunction with the earthpy functions to perform this task.\n",
    "    * ii.\tRead and crop quality assurance files that provides information about the clouds and shadows associated with each image.\n",
    "    * iii.  Read and crop red and infrared bands\n",
    "    * iv.\tUsed conditional statement to remove clouds as there may be some clear days.\n",
    "    * v.\tCalculate mean NDVI using Red and Infrared bands and take mean value for each image.\n",
    "    * vi.\tUltimate goal of this function is to obtain mean NDVI for clear and clouded scenes for both sites that can be saved as a csv file.\n",
    "5.\tDownload images for all sites using glob function and sort all data.\n",
    "6.\tDefine a composite directory and loop through each site directory. \n",
    "    * i.\tObtain sites names in this loop.\n",
    "    * ii.\tObtain list of sub directories for each site.\n",
    "    * iii.  Use geopandas to read shape file.\n",
    "    * iii.\tLoop through each subdirectory to get all bands and sort them.\n",
    "    * iv.\tRecall function to calculate mean NDVI for each image and to store in the list.\n",
    "    * v.    Create a csv file and store in the \"output\" folder.\n",
    "7.  Define second function to reload file, clean data (remove empty cells or NAN values) and create a plot for both sites by grouping on site to develop a time series for 2017.\n",
    "8.  Recall this function to plot cleaned data for HARV and SJER sites and notice seasonal trends.\n",
    "9.\tSummarize your findings on the trends in \"greenness\" with mean NDVI and identify growing seasons.\n",
    "10. Recommendations/ solutions on how weather affects the growing season at each site. \n",
    "suggestions about the best time of the year to capture maximum NDVI values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f47bf73bacf5b85f59a558ef5189b722",
     "grade": false,
     "grade_id": "cell-f749172f0ee10e62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Week 07 and 08 Homework - Automate NDVI Workflow\n",
    "\n",
    "For this assignment, you will write code to generate a plot and an output CSV file of the mean normalized difference vegetation index (NDVI) for two different sites in the United States across one year of data:\n",
    "\n",
    "* San Joaquin Experimental Range (SJER) in Southern California, United States\n",
    "* Harvard Forest (HARV) in the Northeastern United States\n",
    "\n",
    "The data that you will use for this week is available from **earthpy** using the following download: \n",
    "\n",
    "`et.data.get_data('ndvi-automation')`\n",
    "\n",
    "## Assignment Goals\n",
    "\n",
    "Your goal in this assignment is to create the most efficient and concise workflow that you can that allows for:\n",
    "\n",
    "1. The code to scale if you added new sites or more time periods to the analysis.\n",
    "2. Someone else to understand your workflow.\n",
    "3. The LEAST and most efficient (i.e. runs fast) amount of code that completes the task.\n",
    "\n",
    "### HINTS\n",
    "\n",
    "* Remove values outside of the landsat valid range of values as specified in the metadata, as needed.\n",
    "* Keep any output files SEPARATE FROM input files. Outputs should be created in an outputs directory that is created in the code (if needed) and/or tested for.\n",
    "* It can help to create the plot and CSV first without cleaning the data to deal with cloud, so you can get a hang of the workflow.  Then, you can modify your workflow to include the cleaning of the data to deal with clouds. (There are tests throughout the notebook that can help you check the data!)\n",
    "\n",
    "\n",
    "## Assignment Requirements\n",
    "\n",
    "Your submission to the GitHub repository should include:\n",
    "* This Jupyter Notebook file (.ipynb) with:\n",
    "    * The code to create a plot of mean NDVI across the year:\n",
    "        * NDVI on the x axis and formatted dates on the y for both NEON sites on one figure/axis object\n",
    "    * The **data should be cleaned to remove the influence of clouds**. See the [earthdatascience website for an example of what your plot might look like with and without removal of clouds](https://www.earthdatascience.org/courses/earth-analytics-python/create-efficient-data-workflows/).\n",
    "* One output .csv file that has 3 columns - NDVI, Date and Site Name - with values for SJER and HARV.\n",
    "\n",
    "Your notebook should:\n",
    "* Have at least 2 well documented and well named functions with docstrings.\n",
    "* Include a Markdown cell at the top of the notebook that outlines the overall workflow using pseudocode (i.e. plain language, not code)\n",
    "* Include additional Markdown cells throughout the notebook to describe: \n",
    "    * the data that you used - and where it is from\n",
    "    * how data are being processing\n",
    "    * how the code is optimized to run fast and be more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:14.778144Z",
     "start_time": "2020-05-04T21:06:13.866243Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "874f87bc6915adfc58f1e139a8bd27fb",
     "grade": false,
     "grade_id": "cell-75e446b2a4a1bfaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograding imports - do not modify this cell\n",
    "import matplotcheck.autograde as ag\n",
    "import matplotcheck.notebook as nb\n",
    "import matplotcheck.timeseries as ts\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages to synthesize and plot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.483775Z",
     "start_time": "2020-05-04T21:06:14.779664Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b64b778824b5a8ad2b5e0118eda971b",
     "grade": true,
     "grade_id": "cell-2d2391d865730d53",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide",
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Import needed packages in PEP 8 order\n",
    "# and no unused imports listed (10 points total)\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter \n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import earthpy as et\n",
    "import earthpy.spatial as es\n",
    "import earthpy.mask as em\n",
    "import earthpy.plot as ep\n",
    "from rasterio.plot import plotting_extent\n",
    "from shapely.geometry import box\n",
    "# warnings.simplefilter('ignore')\n",
    "\n",
    "# Handle date time conversions between pandas and matplotlib ######### ADD TO LIST\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Use white grid plot background from seaborn\n",
    "sns.set(font_scale=1.5, style=\"whitegrid\")\n",
    "\n",
    "# Adjust plot parameters throughout notebook \n",
    "mpl.rcParams['figure.figsize'] = (14, 14)\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "\n",
    "### DO NOT REMOVE THIS LINE ###\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dfcfc15f692b0e301884fa211a2641d",
     "grade": false,
     "grade_id": "cell-1aaf26353dbf530d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Figure 1: Plot 1 - Mean NDVI For Each Site Across the Year (50 points)\n",
    "\n",
    "Create a plot of the mean normalized difference vegetation index (NDVI) for the two different sites in the United States across the year: \n",
    "\n",
    "* NDVI on the x axis and formatted dates on the y for both NEON sites on one figure/axis object.\n",
    "* Each site should be identified with a different color in the plot and legend.\n",
    "* The final plot **data should be cleaned to remove the influence of clouds**.\n",
    "* Be sure to include appropriate title and axes labels.\n",
    "\n",
    "You may additional cells as needed for processing data (e.g. defining functions, etc), but be sure to:\n",
    "* follow the instructions in the code cells that have been provided to ensure that you are able to use the sanity check tests that are provided. \n",
    "* include only the plot code in the cell identified for the final plot code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define mask values\n",
    "Masking variable is defined here to remove clouds and shadows from the image.\n",
    "The actual function to mask cloud is defined in the earthpy.\n",
    "The information about clouds is stored in the quality assurance file (qa band) for each scene.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.489735Z",
     "start_time": "2020-05-04T21:06:15.485716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define cloud mask to use for all images\n",
    "high_cloud_confidence = em.pixel_flags[\"pixel_qa\"][\"L8\"][\"High Cloud Confidence\"]\n",
    "cloud = em.pixel_flags[\"pixel_qa\"][\"L8\"][\"Cloud\"]\n",
    "cloud_shadow = em.pixel_flags[\"pixel_qa\"][\"L8\"][\"Cloud Shadow\"]\n",
    "\n",
    "# Define name of masking variable\n",
    "all_masked_values = cloud_shadow + cloud + high_cloud_confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to automate NDVI calculations.\n",
    "1. Read and crop bands required for ndvi calculations using earthpy function. \n",
    "2. apply QA to mask clouds using conditional statement, only if scenes shows clouds.\n",
    "3. calculate mean ndvi for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.502701Z",
     "start_time": "2020-05-04T21:06:15.491699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate mean NDVI with and without clouds.\n",
    "def open_band_ndvi_nbr(bands, site_bounds):\n",
    "    \"\"\" \n",
    "    Function to calculate mean NDVI values from series of \n",
    "    Landsat data sets, as follows:\n",
    "    Use context manager and earthpy functions to: \n",
    "    - open and crop following bands:\n",
    "        i. quality assurance (clouds, shades etc.)\n",
    "        ii. red bands\n",
    "        iii. Infrared bands\n",
    "    - use conditional statement to apply QA layer to each band\n",
    "    - calculate mean ndvi for each satellite pass \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bands : tif files containing qa, red, IR band\n",
    "    band = an integer or number\n",
    "    site_bounds : .shp file\n",
    "        .shp file that defines boundaries of study site\n",
    "   \n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    ndvi_mean : int or float\n",
    "    \n",
    "    Earthpy function normalized dfference is used to calculate\n",
    "    ndvi mean value using cropped, cleaned images.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # use context manager and crop function to open and crop QA\n",
    "    with rio.open(bands[0]) as qa_src:\n",
    "        # this code will run before the loop\n",
    "#         site_bounds = site_bds.to_crs(qa_src.crs)\n",
    "        landsat_qa_crop, landsat_qa_meta = es.crop_image(qa_src, site_bounds)\n",
    "    \n",
    "    # Open and crop Band 3\n",
    "    with rio.open(bands[3]) as band_3_src:\n",
    "        band_3_crop, band_3_meta = es.crop_image(band_3_src, site_bounds)\n",
    "        \n",
    "        # Mask Band 3 using QA layer if there are values to mask\n",
    "        try: \n",
    "            band_3_crop_clean = em.mask_pixels(\n",
    "                arr=band_3_crop, mask_arr=landsat_qa_crop, vals=all_masked_values)             \n",
    "        except:\n",
    "            band_3_crop_clean = ma.masked_array(band_3_crop)   \n",
    "    \n",
    "       \n",
    "    # Open and crop Band 4\n",
    "    with rio.open(bands[4]) as band_4_src:\n",
    "        band_4_crop, band_4_meta = es.crop_image(band_4_src, site_bounds)\n",
    "        \n",
    "        # Mask Band 4 using QA layer if there are values to mask\n",
    "        try: \n",
    "            band_4_crop_clean = em.mask_pixels(\n",
    "                arr=band_4_crop, mask_arr=landsat_qa_crop, vals=all_masked_values)             \n",
    "        except:\n",
    "            band_4_crop_clean = ma.masked_array(band_4_crop)   \n",
    "    \n",
    "    # Open and crop Band 6\n",
    "    with rio.open(bands[6]) as band_6_src:\n",
    "        band_6_crop, band_6_meta = es.crop_image(band_6_src, site_bounds)\n",
    "        \n",
    "        # Mask Band 6 using QA layer if there are values to mask\n",
    "        try: \n",
    "            band_6_crop_clean = em.mask_pixels(\n",
    "                arr=band_6_crop, mask_arr=landsat_qa_crop, vals=all_masked_values)             \n",
    "        except:\n",
    "            band_6_crop_clean = ma.masked_array(band_6_crop)  \n",
    "#     # define a plotting extent\n",
    "#     extent = plotting_extent(\n",
    "#     crop_band[0], crop_meta[\"transform\"]) \n",
    "    \n",
    "    # Calculate mean NDVI\n",
    "    ndvi = es.normalized_diff(band_4_crop_clean, \n",
    "                                    band_3_crop_clean).mean()\n",
    "    \n",
    "    nbr = es.normalized_diff(band_4_crop_clean, \n",
    "                                    band_6_crop_clean).mean()\n",
    "    return ndvi, nbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to automate data plotting\n",
    "1. read csv file\n",
    "2. convert to pandas dataframe\n",
    "3. clean data by removing missing values.\n",
    "4. group data for each 'site' using groupby function of panda\n",
    "5. Define date format.\n",
    "6. define x and y labels and legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.513664Z",
     "start_time": "2020-05-04T21:06:15.504697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to plot site NDVI both with and without QA\n",
    "def plot_NDVI(fdir, fname, na_str, title):\n",
    "    \"\"\"\n",
    "    Plots mean ndvi per site per satellite pass by\n",
    "    reading in a .csv file as a pandas dataframe.\n",
    "    Function includes ability to plot any .csv containing\n",
    "    three columns named as below:\n",
    "    columns:   site, date and mean_ndvi.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fdir : str, path\n",
    "        Object of directory path or actual file path\n",
    "        to a directory containing .csv files.\n",
    "        e.g. fdir = glob(os.path.join(path))\n",
    "\n",
    "    fname: str\n",
    "        String of .csv file name.\n",
    "\n",
    "    na_string: str\n",
    "        The string associated with the NA data in the dataset,\n",
    "        e.g. na, NaN, None, --, etc.\n",
    "\n",
    "    title: str\n",
    "        Object of title or actual title describing plot.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    plot :\n",
    "        Printed plot with legend.\n",
    "    \"\"\"\n",
    "    # Import .csv as pd.Dataframe for plotting\n",
    "    df = pd.read_csv(os.path.join(fdir, fname), parse_dates=[\n",
    "                     'date'], index_col=['date'], na_values=na_str)\n",
    "\n",
    "    # Drop NAs\n",
    "    df = df[['site', 'mean_ndvi']].dropna()\n",
    "\n",
    "    # Parse mean_ndvi by site\n",
    "    groups = df.groupby('site')\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plt.suptitle(title, fontsize=16, y=1.0, ha='center')\n",
    "\n",
    "    for sites, group in groups:\n",
    "        ax.plot(group.index.values,\n",
    "                group['mean_ndvi'].values, \n",
    "                ls='-', marker='o', \n",
    "                label=sites)\n",
    "\n",
    "    # Define the date format\n",
    "    date_form_b = DateFormatter(\"%b\")\n",
    "    ax.xaxis.set_major_formatter(date_form_b)\n",
    "\n",
    "    # Define axes and legend\n",
    "    plt.xlabel(\"Month\", fontsize=15)\n",
    "    plt.ylabel(\"Mean NDVI\", fontsize=15)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.519624Z",
     "start_time": "2020-05-04T21:06:15.515655Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Download data\n",
    "# key_name = 'ndvi-automation' \n",
    "\n",
    "# if key_name is None:\n",
    "#     print(\"please provide key name as a string to download data\")\n",
    "# else:\n",
    "#     print(\"downloading data.\")\n",
    "#     download_path = et.data.get_data(key=key_name)\n",
    "    \n",
    "# print(\"Here is your path to view data: \", download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Working Directory to \"earth-analytics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.529598Z",
     "start_time": "2020-05-04T21:06:15.521619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bina_\\\\earth-analytics'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set working directory using conditional statement\n",
    "workdir_path = (os.path.join(et.io.HOME, \"earth-analytics\"))\n",
    "\n",
    "if os.path.exists(workdir_path):\n",
    "    os.chdir(workdir_path)\n",
    "else:\n",
    "    print(\"Path does not exist yet, but making it now!\")\n",
    "    os.mkdir(workdir_path)\n",
    "    os.chdir(workdir_path)\n",
    "\n",
    "# Check working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create base path to retrieve data set \n",
    "download data using glob function and sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.535581Z",
     "start_time": "2020-05-04T21:06:15.531593Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_sites contain path to both sites \n",
    "base_path = os.path.join(\"data\",\"sd_fires_2014\", \"sites\")\n",
    "\n",
    "# # Define list of directories in \"sites\"\n",
    "# all_sites = glob(os.path.join(base_path, \"*/\"))\n",
    "# all_sites.sort()\n",
    "# all_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.582457Z",
     "start_time": "2020-05-04T21:06:15.537577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 262667.59617768 -555228.62003459  269261.95448877 -550853.2180618 ]\n",
      "(1, 2)\n",
      "{'init': 'epsg:3310'}\n",
      "COCOS_fire_bdy: [ 263495.8162 -542742.8268  268961.022  -538848.5866]\n",
      "(1, 2)\n",
      "{'init': 'epsg:3310'}\n"
     ]
    }
   ],
   "source": [
    "# Import SJER shape file and visualize it using geopandas.\n",
    "base_path = os.path.join(\"data\",\"sd_fires_2014\", \"sites\")\n",
    "boundary_path = os.path.join(base_path,\n",
    "                                \"BERNARDO\", \"vector\",\n",
    "                              \"bernardo_box.shp\")\n",
    "\n",
    "# Open a shapefile using geopandas and change CRS.\n",
    "bernardo_bounds = gpd.read_file(boundary_path)\n",
    "# sjer_boundary= sjer_bounds.to_crs(epsg=32611)\n",
    "print(bernardo_bounds.total_bounds)\n",
    "print(bernardo_bounds.shape)\n",
    "print(bernardo_bounds.crs)\n",
    "\n",
    "###############\n",
    "boundary_path2 = os.path.join(base_path,\n",
    "                                \"COCOS\", \"vector\",\n",
    "                              \"cocos_box.shp\")\n",
    "cocos_bounds = gpd.read_file(boundary_path2)\n",
    "print(\"COCOS_fire_bdy:\", cocos_bounds.total_bounds)\n",
    "print(cocos_bounds.shape)\n",
    "print(cocos_bounds.crs)\n",
    "\n",
    "# View data attributes and CRS.\n",
    "\n",
    "# print(sjer_boundary.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:15.942741Z",
     "start_time": "2020-05-04T21:06:15.584451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'uint16', 'nodata': 1.0, 'width': 7681, 'height': 7821, 'count': 1, 'crs': CRS.from_epsg(32611), 'transform': Affine(30.0, 0.0, 366885.0,\n",
      "       0.0, -30.0, 3787815.0)}\n",
      "(1, 7821, 7681)\n",
      "EPSG:32611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "from rasterio.plot import plotting_extent\n",
    "# # Define path for Colorado Springs pre-fire data\n",
    "bernardo_path = os.path.join(base_path,  \"BERNARDO\",\n",
    "                    \"landsat-crop\", \"landsat20140509\",\n",
    "                             \"LC08_L1TP_040037_20140509_20170307_01_T1_pixel_qa.tif\")\n",
    "\n",
    "with rio.open(bernardo_path) as bernardo_src:\n",
    "    bernardo_csf = bernardo_src.read()\n",
    "#     clip_extent= [box(*naip_2015_src.bounds)]\n",
    "    bernardo_csf_meta = bernardo_src.meta\n",
    "    extent = plotting_extent(bernardo_src)\n",
    "    fire_boundary= bernardo_bounds.to_crs(bernardo_src.crs)\n",
    "\n",
    "# View and metadata shape \n",
    "print(bernardo_csf_meta)\n",
    "print(bernardo_csf.shape)\n",
    "print(fire_boundary.crs)\n",
    "\n",
    "# cocos_path = os.path.join(base_path,  \"COCOS\",\n",
    "#                     \"landsat-crop\", \"landsat20140509\",\n",
    "#                             \"LC08_L1TP_040037_20140509_20170307_01_T1_pixel_qa.tif\")\n",
    "# with rio.open(cocos_path) as cocos_src:\n",
    "#     cocos_csf = cocos_src.read()\n",
    "# #     clip_extent= [box(*naip_2015_src.bounds)]\n",
    "#     cocos_csf_meta = cocos_src.meta\n",
    "#     extent = plotting_extent(cocos_src)\n",
    "#     fire_boundary= cocos_bounds.to_crs(cocos_src.crs)\n",
    "    \n",
    "# # View and metadata shape \n",
    "# print(cocos_csf_meta)\n",
    "# print(cocos_csf.shape)\n",
    "# print(fire_boundary.crs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire mean ndvi, site and date information and store in a list.\n",
    "To perform above taks two loops are defined. \n",
    "1. Outer loop is a link to site directories.\n",
    "    - acquires sites name\n",
    "    - define a dynamic boundary shape by \n",
    "        a. reading shape files using geopandas in each site directory.\n",
    "        b. defines a variable \"boundary\" used in Function 1 to crop files.\n",
    "        c. acquire all sub directories and sort them. \n",
    "2. Inner loop acquire is a link to subdirectories (tertiary directory).\n",
    "    - acquire dates using index number from the sub directory\n",
    "    - acquire all bands for each sites and sort them.\n",
    "    - recall Function 1 to crop, mask clouds in each image\n",
    "    = this function also calculates mean ndvi values for each scene.\n",
    "    - site, date, mean ndvi values are stored in the composite list for each scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:18.854256Z",
     "start_time": "2020-05-04T21:06:15.943739Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33c3144e9a4453c61c8d1f4043144e8d",
     "grade": false,
     "grade_id": "cell-0dbb343e769a09db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\bina_\\Miniconda3\\envs\\earth-analytics-python\\lib\\site-packages\\pyproj\\crs.py:77: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method.\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140407': {'BERNARDO': (0.09040582703921118, -0.2475226383395815)}, '20140509': {'TOMAHAWK': (0.14547085255696265, -0.27924403633475964)}, '20140525': {'TOMAHAWK': (0.08878406923193419, -0.24463125813229633)}}\n",
      "Mean Burned area: {'20140407': {'BERNARDO': (0.09040582703921118, -0.2475226383395815)}, '20140509': {'TOMAHAWK': (0.14547085255696265, -0.27924403633475964)}, '20140525': {'TOMAHAWK': (0.08878406923193419, -0.24463125813229633)}}\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define list of directories in \"sites\"\n",
    "all_sites = glob(os.path.join(base_path, \"*/\"))\n",
    "all_sites.sort()\n",
    "all_sites\n",
    "\n",
    "# Create an empty lists to store site name, date, and mean ndvi\n",
    "composite_list = []\n",
    "cleaned_landsat_ndvi = {}\n",
    "cleaned_landsat_nbr = {}\n",
    "\n",
    "# Define the directory name, it is an explicit directory.\n",
    "landsat_dir = \"landsat-crop\"\n",
    "\n",
    "# Loop through each site directory\n",
    "# first loop to go to harv and sjer direcotry\n",
    "for site_dir in all_sites:\n",
    "    \n",
    "    # Get site name\n",
    "    site = os.path.basename(os.path.normpath(site_dir)) \n",
    "#     print(site_dir)\n",
    "#     print(site)\n",
    "#     print(date)\n",
    "    \n",
    "    # Define a dynamic boundary path for the site\n",
    "    boundary_path = os.path.join(site_dir, \"vector\", \n",
    "                                 site + \"_box.shp\")\n",
    "\n",
    "#     print(boundary_path)\n",
    "    fire_boundary = gpd.read_file(boundary_path)\n",
    "#     print(fire_boundary)\n",
    "   \n",
    "    # Get a list of subdirectories for that site\n",
    "    new_path = os.path.join(site_dir, landsat_dir)\n",
    "#     print(\"new path:\", new_path)\n",
    "    \n",
    "    # all dirs is acessing all image directory\n",
    "    # inner loop iterates on each of these (23)x2 subdirectories.\n",
    "    all_dirs = glob(new_path + \"/*/\")  \n",
    "    all_dirs.sort()\n",
    "#     print(\"all directories:\", all_dirs)\n",
    "\n",
    "    # Loop through each subdirectory for site\n",
    "    # the second loop opens tif files\n",
    "    for dir in all_dirs:\n",
    "        date = os.path.basename(os.path.normpath(dir))[-8:] \n",
    "#         print(date)\n",
    "        \n",
    "        # Get all bands in each subdirectory for site\n",
    "        site_bands = glob(dir + \"*.tif\")\n",
    "        site_bands.sort()\n",
    "#         print(site_bands)\n",
    "#         with rio.open(site_bands[1]) as src:\n",
    "#         fire_boundary_prj= fire_boundary.to_crs(bernardo_src.crs)\n",
    "        # create plotting extent\n",
    "        with rio.open(site_bands[3]) as src:\n",
    "            extent = plotting_extent(src)\n",
    "            fire_boundary_prj= fire_boundary.to_crs(src.crs)\n",
    "#             print(src.crs)\n",
    "            \n",
    "            # Run function to calculate NDVI for that subdirectory (scene)\n",
    "            ndvi_clean = open_band_ndvi_nbr(bands= \n",
    "                            site_bands, site_bounds= fire_boundary_prj)\n",
    "            \n",
    "            # Run function to calculate Nbr for that subdirectory (scene)\n",
    "            nbr_clean = open_band_ndvi_nbr(bands= \n",
    "                            site_bands, site_bounds= fire_boundary_prj)\n",
    "            \n",
    "#             print(ndvi_clean.mean())\n",
    "            composite_list.append([site, ndvi_clean, date])\n",
    "            # add cleaned ndvi_clean to the dictionary\n",
    "            cleaned_landsat_ndvi[date] = {site: ndvi_clean}\n",
    "            \n",
    "            # add cleaned ndvi_clean to the dictionary\n",
    "            cleaned_landsat_nbr[date] = {site: nbr_clean}\n",
    "            \n",
    "\n",
    "print(cleaned_landsat_ndvi)\n",
    "print(\"Mean Burned area:\", cleaned_landsat_nbr)\n",
    "\n",
    "# # Convert list of values to a data frame\n",
    "# col_names = [\"site\", \"ndvi\", \"date\"]\n",
    "# ndvi_df = pd.DataFrame(composite_list, columns=col_names)\n",
    "# # print(ndvi_df)\n",
    "\n",
    "# # Save as csv file\n",
    "# ndvi_df.to_csv(os.path.join(\"data\", \"sd_fires_2014\", \"outputs\",\n",
    "#                                 \"ndvi_df.csv\"), header=True)\n",
    "# print(\"Landsatcomposite list: \", composite_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:19.012799Z",
     "start_time": "2020-05-04T21:06:18.856218Z"
    },
    "caption": "Plot showing NDVI for each time period at both NEON Sites. In this example the cloudy pixels were removed using the pixel_qa cloud mask. Notice that this makes a significant different in the output values. Why do you think this difference is so significant?",
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d5ed3aac35f986672b4e0228debc567",
     "grade": false,
     "grade_id": "cell-10287141af53f723",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'20140409'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c01ecf5686b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleaned_landsat_ndvi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'20140409'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BERNARDO'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: '20140409'"
     ]
    }
   ],
   "source": [
    "cleaned_landsat_ndvi['20140409']['BERNARDO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:19.018783Z",
     "start_time": "2020-05-04T21:06:13.882Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_landsat_ndvi[\"20140409\"][\"BERNARDO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:19.020781Z",
     "start_time": "2020-05-04T21:06:13.885Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "ep.plot_bands(cleaned_landsat_ndvi[\"20140509\"][\"COCOS\"],\n",
    "              ax=ax1,\n",
    "              extent = extent,\n",
    "             vmin=-1.0, vmax=1.0,\n",
    "              cmap='PiYG',\n",
    "             scale = False)\n",
    "\n",
    "fire_crop_bound.plot(ax=ax2, color=\"None\",\n",
    "                     edgecolor=\"white\", linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:19.021776Z",
     "start_time": "2020-05-04T21:06:13.887Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef0928ee835d8195992e459de791292e",
     "grade": true,
     "grade_id": "cell-963612e7cefa2a5e",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Ignore this cell for the autograding tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Optimization\n",
    "1. I have defined two functions which facilitates repetitive tasks.\n",
    "    * Image processing, and NDVI calculations.\n",
    "    * Plotting data using multiple csv files.\n",
    "2. I have used two loops to automatically acquire directories name, sort subdirectory, acquire shape files to crop images.\n",
    "3. I have used inner loop to open bands and calculate mean NDVI for each image.\n",
    "4. I have used one composite list to store all data values, site and date information.\n",
    "5. The list was changed to a dataframe and saved as csv file to recall later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76525913b95c76f21652c352d1683bc9",
     "grade": false,
     "grade_id": "cell-a198c5110802f106",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Question 1 (10 points)\n",
    "\n",
    "Imagine that you are planning NEONâ€™s upcoming flight season to capture remote sensing data in these locations and want to ensure that you fly the area when the vegetation is the most green.\n",
    "\n",
    "When would you recommend the flights take place for each site? \n",
    "\n",
    "Answer the question in 2-3 sentences in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7471e9001235e5edcdedd8332009371",
     "grade": true,
     "grade_id": "cell-f205bcd0ff1dbea4",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Ans: Final plot with cleaned mean NDVI values for 2017 indicates that July-Oct. are best suited for flights to capture maximum \"greenness\" for the HARV and March/April for the SJER sites.\n",
    "Our recommendation should be based on statistical analysis of multiple year of data for any site. Regard to SJER site, I have my reservations to recommend Mar/April period to measure maximum NDVI for reasons given in the additional note.\n",
    "\n",
    "**Additional note** \n",
    "The  SJER site is located in the California which is impacted heavily by the El-Nino Southern Oscillations (ENSO). ENSO is a global weather pattern that begins in the Pacific. The ENSO period results in reduced precipitation in California. The precipitation data indicates that 2017 was an exceptionally dry and hot year and it is not a true representative of the normal vegetation growth in California. It is worth checking vegetation indices (NDVI) during La Nina seasons-a weather pattern with more than usual rainfall in the California, to obtain a real picture of vegetation growth and seasonality over both ENSO and La Nina seasons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abc9ac9bb09fe213b0562a5e7327e17d",
     "grade": false,
     "grade_id": "cell-4ad103ec7e34a981",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Question 2 (10 points)\n",
    "\n",
    "How could you modify your workflow to look at vegetation changes over time in each site? \n",
    "\n",
    "Answer the question in 2-3 sentences in the Markdown cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e42d74b1a5945e05700be2af0040aa41",
     "grade": true,
     "grade_id": "cell-97ba4d89e2b11832",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The NOAA has been collecting earth's data using various Landsat satellites for over 40 years, there are multiple ways to look at vegetation changes over time. \n",
    "1. Plotting time series (2-3 decades of data) of mean NDVI by taking a composite value for each year (yearly mean NDVI) along with error bars or min-max NDVI values. This plot would provide information on the long term trends and how climate change (rising temperature due to the increase in carbon dioxide) is affecting plant growth. There is an interesting paradox that high CO2 will increase photosynthesis. However, proponent of this hypothesis have not into account the impact of heat and availability of moisture/water during growing season. Plotting long term record of mean NDVI, temperature, and soil moisture will help to answer some of the climate related impacts on the vegetation.  \n",
    "2. Plotting duration of growth growth season over 2-3 decades. This plot would provide us information about changes in growing season such as early onset of growth season and/ or late onset of senescence (loss of green pigments in plants)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c241692b4bc3c65d9b836fdfb1106880",
     "grade": false,
     "grade_id": "cell-85bcd71743cd3d2c",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Do not edit this cell! (40 points)\n",
    "\n",
    "The notebook includes:\n",
    "* at least 2 well documented and well named functions with appropriately formatted docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:19.022773Z",
     "start_time": "2020-05-04T21:06:13.892Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23c12dce1885b13e71b582aa4bc1bb3d",
     "grade": true,
     "grade_id": "cell-622a5e7598f525d1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Last sanity check before creating your plot\n",
    "\n",
    "# Ensure that your dataframe is named 'ndvi_ts'\n",
    "# and that the columns called: mean_ndvi and site\n",
    "\n",
    "# These tests are not graded.\n",
    "\n",
    "# Ensure the data is stored in a dataframe.\n",
    "try:\n",
    "    assert isinstance(ndvi_df, pd.DataFrame)\n",
    "    print('Your data is stored in a DataFrame!')\n",
    "except AssertionError:\n",
    "    print('It appears your data is not stored in a DataFrame. ',\n",
    "          'To see what type of object your data is stored in, check its type with type(object)')\n",
    "\n",
    "# Check that dataframe contains the appropriate number of NAN values\n",
    "try:\n",
    "    assert ndvi_df.isna().sum()['mean_ndvi'] == 15\n",
    "    print('Correct number of masked data values!')\n",
    "except AssertionError:\n",
    "    print('The amount of null data in your dataframe is incorrect.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:19.023770Z",
     "start_time": "2020-05-04T21:06:13.895Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a8f991ca5aaf4046358e60015def359",
     "grade": true,
     "grade_id": "cell-d86b87f27caf0d5e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your dataframe before cleaning the data to deal with clouds. \n",
    "# Ensure that your dataframe is named 'ndvi_ts_unclean'\n",
    "# and that the columns called: mean_ndvi and site\n",
    "\n",
    "# These tests are not graded.\n",
    "# This is for data that hasn't been cleaned yet to deal \n",
    "# with clouds and serves as a half way sanity check.\n",
    "\n",
    "# Ensure the data is stored in a dataframe.\n",
    "try:\n",
    "    assert isinstance(ndvi_ts_unclean, pd.DataFrame)\n",
    "    print('Your data is stored in a DataFrame!')\n",
    "except AssertionError:\n",
    "    print('It appears your data is not stored in a DataFrame. ',\n",
    "          'To see what type of object your data is stored in, check its type with type(object)')\n",
    "\n",
    "# Ensure there are the correct amount of total entries in the dataframe.\n",
    "try:\n",
    "    assert len(ndvi_ts_unclean) == 46\n",
    "    print('You have the correct number of data values!')\n",
    "except AssertionError:\n",
    "    print('You do not have the correct amount of data stored in your DataFrame.')\n",
    "\n",
    "# Ensure there are the correct amount of entries for each site.\n",
    "try:\n",
    "    assert all(\n",
    "        [site_count == 23 for site_count in ndvi_ts_unclean['site'].value_counts()])\n",
    "    print('You have the correct amount of both sites!')\n",
    "except AssertionError:\n",
    "    print('One of your sites is either missing data or has extra data.')\n",
    "\n",
    "# Ensure the minimum and maximum values in the mean_ndvi column are correct.\n",
    "try:\n",
    "    ndvi_min, ndvi_max = ndvi_ts_unclean['mean_ndvi'].min(), ndvi_ts_unclean['mean_ndvi'].max()\n",
    "    assert ndvi_min == -0.0020918187219649553 and ndvi_max == 0.8629496097564697\n",
    "    print('The minimum and maximum values in your ndvi_mean column are correct!')\n",
    "except AssertionError:\n",
    "    print('The minimum and maximum values in your ndvi_mean column are incorrect.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T21:06:19.023770Z",
     "start_time": "2020-05-04T21:06:13.897Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a02997938bd2b24724ceb216662a761",
     "grade": false,
     "grade_id": "cell-1e463322f3d6c1a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is not required but highly encouraged to break down the workflow.\n",
    "\n",
    "# Create dataframe of NDVI without cleaning data to deal with clouds\n",
    "\n",
    "# Important: to use the ungraded tests below as a sanity check, \n",
    "# name your dataframe 'ndvi_ts_unclean' and the columns: mean_ndvi and site\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ef6cf57aba4c34ed5ca3a8d5b45e874",
     "grade": false,
     "grade_id": "cell-041e86d8b5b991f4",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Do not edit this cell! (20 points)\n",
    "\n",
    "The notebook includes:\n",
    "* a Markdown cell at the top of the notebook that outlines the overall workflow using pseudocode (i.e. plain language, not code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b48fbf1cd4be9220acbb96e3db0132a7",
     "grade": false,
     "grade_id": "cell-1608890dfac4f8f1",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Do not edit this cell! (20 points)\n",
    "\n",
    "The notebook includes:\n",
    "* additional Markdown cells throughout the notebook to describe: \n",
    "    * the data that you used - and where it is from\n",
    "    * how data are being processing\n",
    "    * how the code is optimized to run fast and be more concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89c19e496e9b1aa16655dbf9b5ab90c7",
     "grade": false,
     "grade_id": "cell-5a25fd4ff6edeea9",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Do not edit this cell! (20 points)\n",
    "\n",
    "The notebook will also be checked for overall clean code requirements as specified at the **top** of this notebook. Some of these requirements include (review the top cells for more specifics): \n",
    "\n",
    "* Notebook begins at cell [1] and runs on any machine in its entirety.\n",
    "* PEP 8 format is applied throughout (including lengths of comment and code lines).\n",
    "* No additional code or imports in the notebook that is not needed for the workflow.\n",
    "* Notebook is fully reproducible. This means:\n",
    "   * reproducible paths using the os module.\n",
    "   * data downloaded using code in the notebook.\n",
    "   * all imports at top of notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7d559d3ecf6c2a7588699ea045a90fb",
     "grade": false,
     "grade_id": "cell-bda986619c137907",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Do not edit this cell! (20 points)\n",
    "\n",
    "In addition to this notebook, the submission to the GitHub repository includes:\n",
    "* One output .csv file that has 3 columns - NDVI, Date and Site Name - with values for SJER and HARV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
